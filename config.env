# uncomment and adapt to pre-select model rather than dynamically fetching one 
# NEXT_MODEL_PATH="microsoft/phi-2"
# NEXT_MODEL_REVISION="main"
# NEXT_MODEL_PRECISION="float16"

CHAINS="HandsOn,ReflectBeforeRun" # "HandsOn"
MODELKWARGS='[{temperature: .3, top_k: 100, top_p: .95},{temperature: 0},{temperature: 0, use_beam_search: true, best_of: 2, n: 1}]'  # YAML format
TASKS="logiqa,logiqa2,lsat-ar,lsat-rc,lsat-lr"
TRUST_REMOTE_CODE=true
MAX_LENGTH=4096 # make sure that this is NOT greater than the derived max_model_len (in model's config.json), see also https://github.com/vllm-project/vllm/issues/1559#issuecomment-1797100930 
DO_BASEEVAL=true
MAX_MODEL_PARAMS=10 # max number of params (B) of evaluated model
NUM_GPUS=1

CREATE_PULLREQUESTS=true